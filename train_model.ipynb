{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tksen\\AppData\\Local\\Temp\\ipykernel_16132\\3256193227.py:2: DeprecationWarning: The '__version__' attribute is deprecated and will be removed in Flask 3.1. Use feature detection or 'importlib.metadata.version(\"flask\")' instead.\n",
      "  flask.__version__\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.1.0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import flask\n",
    "flask.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with header names from the CSV file.\n",
    "# Ensure that 'Loan_data_ver2.csv' is in the same directory as this script.\n",
    "data = np.genfromtxt('Loan_data_ver2.csv', delimiter=',', names=True, dtype=None, encoding='utf-8')\n",
    "\n",
    "# Get the header names (fields) from the structured array.\n",
    "header_names = data.dtype.names\n",
    "\n",
    "# Assume that the last column is the target variable.\n",
    "# Create the feature array by stacking all columns except the last.\n",
    "X = np.column_stack([data[name] for name in header_names[:-1]])\n",
    "\n",
    "# Extract the target variable.\n",
    "y = data[header_names[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Graduate', '5849', '-1', '1'], dtype='<U12')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Step 2: Process Features\n",
    "# ---------------------------\n",
    "# Since we don't have pandas DataFrame, we need to work with numpy arrays.\n",
    "# You need to know which columns are numeric and which are categorical.\n",
    "# For this example, let's assume:\n",
    "# - Numeric columns: indices 0, 1, 2\n",
    "# - Categorical columns: indices 3, 4\n",
    "\n",
    "# # Extract numeric and categorical features using numpy indexing\n",
    "# X_numeric = X[:, [2, 3, 4]].astype(float)  # converting to float if needed\n",
    "# X_categorical = X[:, [0, 1]]  # these remain as strings\n",
    "\n",
    "# Extract numeric and categorical features using numpy indexing\n",
    "X_numeric = X[:, [2, 3, 4]].astype(float)  # Ensure numeric columns are float\n",
    "X_categorical = X[:, [0, 1]].astype(object)  # Convert to object dtype for categorical processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Step 3: Build Preprocessing Pipelines\n",
    "# ---------------------------\n",
    "# Numeric pipeline: impute missing values and scale features\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # or use KNNImputer\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: impute missing values and one-hot encode\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Since ColumnTransformer expects column indices or names, we'll build separate pipelines.\n",
    "# We'll fit each pipeline on the corresponding numpy arrays.\n",
    "X_numeric_transformed = numeric_pipeline.fit_transform(X_numeric)\n",
    "X_categorical_transformed = categorical_pipeline.fit_transform(X_categorical).toarray()\n",
    "\n",
    "#X_categorical_transformed = categorical_pipeline.fit_transform(X_categorical).toarray()\n",
    "\n",
    "# Combine the processed numeric and categorical features using numpy concatenation\n",
    "X_processed = np.concatenate([X_numeric_transformed, X_categorical_transformed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline saved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Step 4: Split Data\n",
    "# ---------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 5: Build and Train the Model\n",
    "# ---------------------------\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 6: Save the Pipeline\n",
    "# ---------------------------\n",
    "# Since we manually processed the data, you can save both the preprocessing components and the model.\n",
    "# Here, we save a dictionary containing the pipelines and the model.\n",
    "pipeline = {\n",
    "    'numeric_pipeline': numeric_pipeline,\n",
    "    'categorical_pipeline': categorical_pipeline,\n",
    "    'categorical_indices': [0, 1],  # indices for categorical features in original X\n",
    "    'numeric_indices': [2, 3, 4],\n",
    "    'model': model\n",
    "}\n",
    "\n",
    "with open('full_pipeline', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "print(\"Pipeline saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
